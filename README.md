# ğŸ¤– Mini RAG Chatbot â€“ Local AI Assistant

> **An intelligent local assistant that answers questions from PDF documents using a Retrieval-Augmented Generation (RAG) architecture.**  
> Built with **LangChain**, **Ollama (Mistral)**, **ChromaDB**, and **Gradio**.

---

## ğŸ¯ Project Goal

CrÃ©er un **assistant intelligent local** capable de rÃ©pondre Ã  des questions en langage naturel Ã  partir dâ€™un document PDF.  
Le projet met en Å“uvre une architecture **Retrieval-Augmented Generation (RAG)** pour combiner **recherche sÃ©mantique** et **gÃ©nÃ©ration de texte**, garantissant des **rÃ©ponses prÃ©cises, pertinentes et contextuelles**.

---

## ğŸ§© Tech Stack

| Category | Tools / Frameworks |
|-----------|--------------------|
| **Langage** | Python |
| **Framework RAG** | LangChain |
| **LLM Local** | Ollama (Mistral) |
| **Vector Store** | ChromaDB |
| **Interface Web** | Gradio |
| **Conteneurisation** | Docker |
| **Embeddings** | nomic-embed-text |
| **Alternative Vector Store** | FAISS |

---

## ğŸ—ï¸ Architecture Overview

**Pipeline RAG complet :**

1. ğŸ“¥ Chargement et dÃ©coupage du PDF  
2. ğŸ§® GÃ©nÃ©ration dâ€™embeddings via *nomic-embed-text*  
3. ğŸ§  Stockage et recherche sÃ©mantique avec *ChromaDB*  
4. ğŸ’¬ GÃ©nÃ©ration de rÃ©ponses par *Mistral* (via Ollama)  
5. ğŸŒ Interface utilisateur simple et interactive avec *Gradio*  

```text
User Query â†’ PDF Splitter â†’ Embeddings â†’ Vector Store â†’ RAG Chain â†’ Answer

ğŸ§  Models Used

LLM : Mistral
 dÃ©ployÃ© localement via Ollama

Embeddings : nomic-embed-text â€” modÃ¨le lÃ©ger et performant pour la vectorisation rapide

ğŸ“„ Data Source

Fichiers PDF uploadÃ©s par lâ€™utilisateur

Le contenu est :

dÃ©coupÃ© et nettoyÃ©,

vectorisÃ© et stockÃ© localement,

interrogÃ© en toute confidentialitÃ© et rapiditÃ©.

âš™ï¸ Installation & Usage
1ï¸âƒ£ Cloner le projet
git clone https://github.com/<your_username>/mini_rag.git
cd mini_rag

2ï¸âƒ£ CrÃ©er et activer lâ€™environnement virtuel
python -m venv .venv
# Sur Windows :
.venv\Scripts\Activate.ps1
# Sur macOS / Linux :
source .venv/bin/activate

3ï¸âƒ£ Installer les dÃ©pendances
pip install -r requirements.txt

4ï¸âƒ£ Lancer lâ€™application
python app.py


Lâ€™application sâ€™exÃ©cute ensuite en local sur :
ğŸ‘‰ http://127.0.0.1:7860

Vous pouvez y uploader un PDF et interagir avec le chatbot directement depuis lâ€™interface web.
