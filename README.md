<p align="center">
  <img src="assets/logo.png" alt="Mini RAG Chatbot Logo" width="200"/>
</p>

<h1 align="center"> Mini RAG Chatbot â€“ Local AI Assistant</h1>

<p align="center">
  <em>An intelligent local assistant that answers questions from PDFs using Retrieval-Augmented Generation (RAG)</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python 3.11 Badge"/>
  <img src="https://img.shields.io/badge/LangChain-%2300BFA5.svg?style=for-the-badge&logoColor=white" alt="LangChain Badge"/>
  <img src="https://img.shields.io/badge/Ollama%20%2F%20Mistral-8A2BE2?style=for-the-badge&logo=openai&logoColor=white" alt="Ollama Mistral Badge"/>
  <img src="https://img.shields.io/badge/ChromaDB-orange?style=for-the-badge&logo=databricks&logoColor=white" alt="ChromaDB Badge"/>
</p>

---

## ğŸ“š Table of Contents
| Section | Description |
|----------|--------------|
| [ğŸ¯ Project Goal](#-project-goal) | Overview and objectives |
| [ğŸ§© Tech Stack](#-tech-stack) | Tools and frameworks used |
| [ğŸ—ï¸ Architecture Overview](#ï¸-architecture-overview) | System design and pipeline |
| [ğŸ§  Models Used](#-models-used) | LLM and embeddings details |
| [ğŸ“„ Data Source](#-data-source) | Input document and processing |
| [âš™ï¸ Installation & Usage](#ï¸-installation--usage) | Setup and execution guide |
| [ğŸ³ Docker](#-docker-optionnel) | Containerization instructions |
| [ğŸš€ Features](#-features) | Key capabilities |
| [ğŸ“¦ Project Structure](#-project-structure) | Folder and file organization |
| [ğŸ’¡ Next Steps](#-next-steps) | Future improvements |
| [ğŸ‘¤ Author](#-author) | Credits and contact |
| [ğŸ“ License](#-license) | Licensing details |


---

## ğŸ¯ Project Goal

Create a **local intelligent assistant** capable of answering natural language questions based on a PDF document.
The project implements a **Retrieval-Augmented Generation (RAG)** architecture to combine **semantic search** and **text generation**, ensuring **accurate**, **relevant**, and **context-aware responses**. 

__

CrÃ©er un **assistant intelligent local** capable de rÃ©pondre Ã  des questions en langage naturel Ã  partir dâ€™un document PDF.  
Le projet met en Å“uvre une architecture **Retrieval-Augmented Generation (RAG)** pour combiner **recherche sÃ©mantique** et **gÃ©nÃ©ration de texte**, garantissant des **rÃ©ponses prÃ©cises, pertinentes et contextuelles**.

---

## ğŸ§© Tech Stack

| Category | Tools / Frameworks |
|-----------|--------------------|
| **Langage** | Python |
| **Framework RAG** | LangChain |
| **LLM Local** | Ollama (Mistral) |
| **Vector Store** | ChromaDB |
| **Interface Web** | Gradio |
| **Conteneurisation** | Docker |
| **Embeddings** | nomic-embed-text |
| **Alternative Vector Store** | FAISS |

---

## ğŸ—ï¸ Architecture Overview

```text
Pipeline RAG complet :

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        User Uploads       â”‚
 â”‚         PDF File          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        PDF Loader         â”‚
 â”‚ - Reads & splits content  â”‚
 â”‚ - Cleans and prepares textâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     Embeddings Engine     â”‚
 â”‚ - nomic-embed-text model  â”‚
 â”‚ - Converts text â†’ vectors â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚       Vector Store        â”‚
 â”‚         ChromaDB          â”‚
 â”‚ - Stores embeddings       â”‚
 â”‚ - Enables semantic search â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     RAG Pipeline (LLM)    â”‚
 â”‚ - Mistral via Ollama      â”‚
 â”‚ - Retrieves context + gen â”‚
 â”‚ - Produces final answer   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚         Gradio UI         â”‚
 â”‚ - Chat interface          â”‚
 â”‚ - Displays responses      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Query â†’ PDF Splitter â†’ Embeddings â†’ Vector Store â†’ RAG Chain â†’ Answer

```
## ğŸš€ Features

- **Local RAG Intelligence:** Combines semantic retrieval and generative AI for accurate, contextual answers from PDFs.  
- **PDF Understanding:** Automatically extracts, splits, and processes PDF documents for question answering.  
- **End-to-End RAG Pipeline:** Integrates LangChain + ChromaDB + Mistral (via Ollama) for retrieval-augmented generation.  
- **100% Local Execution:** All data and inference happen locally â€” ensuring complete privacy and control.  
- **Interactive Gradio UI:** Simple and responsive web interface for seamless chatbot interactions.  
- **Fast Vector Search:** Uses optimized embeddings with `nomic-embed-text` and ChromaDB for efficient retrieval.  
- **Docker-Ready Deployment:** Fully containerized for quick setup and reproducible environments.  
- **Modular Codebase:** Clean architecture enabling easy integration of new models or vector databases.  
- **Scalable Extensions:** Future-proof design â€” ready for multi-document support, advanced analytics, and new LLMs.  


## ğŸ§  Models Used

Â° LLM : Mistral dÃ©ployÃ© localement via Ollama

Embeddings : nomic-embed-text â€” modÃ¨le lÃ©ger et performant pour la vectorisation rapide

Â° Data Source :Fichiers PDF uploadÃ©s par lâ€™utilisateur

Le contenu est : dÃ©coupÃ© et nettoyÃ©, vectorisÃ© et stockÃ© localement, interrogÃ© en toute confidentialitÃ© et rapiditÃ©.

## âš™ï¸ Installation & Usage
1ï¸âƒ£ Cloner le projet
git clone https://github.com/<your_username>/mini_rag.git
cd mini_rag

2ï¸âƒ£ CrÃ©er et activer lâ€™environnement virtuel
python -m venv .venv
# Sur Windows :
.venv\Scripts\Activate.ps1
# Sur macOS / Linux :
source .venv/bin/activate

3ï¸âƒ£ Installer les dÃ©pendances
pip install -r requirements.txt

4ï¸âƒ£ Lancer lâ€™application
python app.py


Lâ€™application sâ€™exÃ©cute ensuite en local sur :
ğŸ‘‰ http://127.0.0.1:7860

Vous pouvez y uploader un PDF et interagir avec le chatbot directement depuis lâ€™interface web.
